from sentence_transformers import SentenceTransformer, util
import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from models.llm_model import get_llm
# from test.queries_answer import questions, answers
from config.combined import process_query
from modules.extraction_topk import query_data
import matplotlib.pyplot as plt
import numpy as np
from matplotlib.ticker import MaxNLocator


# Load the pre-trained Sentence-BERT model
model = SentenceTransformer('all-MiniLM-L6-v2')

questions = [
    "How can I review my bills?",
    "Are you guys offering insurance in case of damage",
    "Do you guys offer preowned phone?",
    "How can I check my device is approved device?",
    "Do you guys have pre-owned phone for sale",
    "I got problem with cards",
    "Do you offer data Plan for Europe?",
    "Which area has enhanced calling?",
    "Is there any recent update of Samsung",
    "Can I file an issue thirough Device Protection",
    "Do you guys have a switcher offer right now",
    "Can you walk us through Data speed is this reduced?",
    "When software update will be announced?",
    "How can we powers off this phone?",
    "Is Device protection service free",
    "How to contact the sales?",
    "Do you have any PIN codes to unlock the device",
    "Do guys support the GPS+cellular version",
    "Where can we review switcher offer",
    "How can we identify partner coverage",
    "Where we can contact Solutions Team"
]

answers = [
    "You can send us the email and we can review the bills for you",
    "we offer insurance incase of damage to your phone",
    "Yes we do offer preowned phone you can visit our website http:/preowned.com",
    "To check whether your phone is approved we recommend you to visiting retail for activation",
    "We do have pre-owned phones and other items for sales",
    "The cards are non refundable. Do you mind sending me your email so I can check",
    "We do offer data plan for Europe at $35 for 50MB",
    "The enhanced calling is available in the Iowa area",
    "Samsung has not announced any recent update",
    "Filing an issue through Device protection is the cheapest option I would not recommend you",
    "Yes we do have a switcher offer right now for 65 bucks",
    "Data speed is reduced after 22GBs on our unlimited plans",
    "A software update will be released beginning tomorrow",
    "Hold both volumn down button and power off button for a while",
    "Device protection + is a service that protects your phone",
    "Sales can be contacted at 888-289-8722.",
    "I recommend searching google for information because we do not have any PIN codes to unlock the device.",
    "We do not support the GPS+cellular version right now",
    "You can review switcher offer https://t.co/ud8X6C9lKx",
    "If you select voice and zoom in you will see grid lines through the state of Michigan. This means it is partner coverage.",
    "You can speak with our Solutions Team at 888-944-9400 to assist with this."
]

print(len(questions),len(answers))

language_model = get_llm()


def calculate_semantic_similarity(reference: str, generated_output: str) -> float:
    """
    Calculate semantic similarity using Sentence-BERT.

    Args:
    reference (str): The reference or expected sentence.
    generated_output (str): The sentence generated by the model/system.

    Returns:
    float: The semantic similarity score.
    """
    # Compute embeddings
    embedding1 = model.encode(reference, convert_to_tensor=True)
    embedding2 = model.encode(generated_output, convert_to_tensor=True)
    
    # Compute cosine similarity
    similarity = util.cos_sim(embedding1, embedding2)
    
    return similarity.item()

prompt_template = """
    You are an intelligent assistant who has access to relevant context for better answering questions. 
    Below is some context information that may help you understand the query better:

    Context:
    {context}

    Now, here is the user's query:

    Query:
    {query}

    Please provide a short and simple response based on the context above. Response from the company point of view
    and yeah answer in tone of someone really from the company avoid unecessary phreses
    """
    

semantic_score_special_arch_scores = []
def semantic_score_special_Arch():
        # Step: Calculate BLEU score for the generated response
    cosine_similarities = 0
    for i, q in enumerate(questions):
        # Reference answer for the current question
        reference_answer = answers[i]
        
        generated_context = process_query(q)   
        if isinstance(generated_context, list) and len(generated_context) > 0 and isinstance(generated_context[0], tuple):
            top_3_texts = [item[0] for item in sorted(generated_context, key=lambda x: x[1], reverse=True)[:3]]
        else:
            top_3_texts = []

        print('************************************Top 3 are*****************************','\n')
        print(top_3_texts)
        
        context = "\n".join(top_3_texts)
        query = q
        
        prompt = prompt_template.format(context=context, query=query)

        generated_answer = language_model(prompt)
        
        # print('Answer',generated_answer.content)
        # print('Refrence Answer',reference_answer)
        
        # Calculate BLEU score
        cosine_similarity = calculate_semantic_similarity(reference_answer, generated_answer.content)
        # Print the BLEU score for the current question
        print(f"BLEU score for Question {i+1}: {cosine_similarity:.4f}")
        semantic_score_special_arch_scores.append(cosine_similarity)
        
        cosine_similarities = cosine_similarities + cosine_similarity
    
    return cosine_similarities/21
        

print('The average cosine similarity is',semantic_score_special_Arch())

semantic_gpt_score = []
def semantic_score_rag():
    cosine_similarities = 0
    for i,q in enumerate(questions):
        generated_context = query_data(q)
                
        matches = generated_context.get('matches', [])
        if matches:
            # Sort matches by score in descending order and extract the top 3
            top_3_texts = [match['metadata']['text'] for match in sorted(matches, key=lambda x: x['score'], reverse=True)[:3]]
        else:
            top_3_texts = []

        # Print the results
        print('************************************Top 3 are in blue*****************************', '\n')
        print(top_3_texts)
                    
        context = "\n".join(top_3_texts)
        print(context)
        query = q
        reference_answer = answers[i]
        
        prompt = prompt_template.format(context=context, query=query)
 
        generated_answer = language_model(q)
        
        # print('Answer',generated_answer.content)
        # print('Refrence aswer',reference_answer)
        
        # Calculate BLEU score
        cosine_similarity = calculate_semantic_similarity(reference_answer, generated_answer.content)
        # Print the BLEU score for the current question
        print(f"BLEU score for Question {i+1}: {cosine_similarity:.4f}")
        semantic_gpt_score.append(cosine_similarity)
        
        cosine_similarities = cosine_similarities + cosine_similarity
    
    return cosine_similarities/21

print('The averge cosine similarity using pure RAG is',semantic_score_rag())


eval_folder = "eval"
os.makedirs(eval_folder, exist_ok=True)

# Example data for demonstration
questions_indices = list(range(1, 22))

# Plot: Comparison of Average Scores
plt.figure(figsize=(8, 5))
methods = ["Special Arch", "GPT"]
avg_scores = [sum(semantic_score_special_arch_scores) / len(semantic_score_special_arch_scores),
              sum(semantic_gpt_score) / len(semantic_gpt_score)]
plt.bar(methods, avg_scores, color=['blue', 'green'])
plt.title("Average Cosine Similarity Comparison")
plt.ylabel("Cosine Similarity")
plt.savefig(os.path.join(eval_folder, "average_scores_comparison.png"))
plt.close()

# Plot: Per-Question Performance
plt.figure(figsize=(12, 6))
plt.plot(questions_indices, semantic_score_special_arch_scores, marker='o', label="Special Arch", color='blue')
plt.plot(questions_indices, semantic_gpt_score, marker='s', label="GPT", color='green')
plt.title("Cosine Similarity Per Question")
plt.xlabel("Question Index")
plt.ylabel("Cosine Similarity")
plt.legend()
plt.grid(True)
plt.savefig(os.path.join(eval_folder, "per_question_performance.png"))
plt.close()

os.listdir(eval_folder)  # To confirm the files are saved in the eval folder
